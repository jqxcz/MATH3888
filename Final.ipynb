{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import collections\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "# some basic settings for plotting figures\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 32}\n",
    "\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 90.0M  100 90.0M    0     0  4644k      0  0:00:19  0:00:19 --:--:-- 6648k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1900k  100 1900k    0     0  1121k      0  0:00:01  0:00:01 --:--:-- 1120k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1957k  100 1957k    0     0  1173k      0  0:00:01  0:00:01 --:--:-- 1173k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 95667  100 95667    0     0   275k      0 --:--:-- --:--:-- --:--:--  275k\n"
     ]
    }
   ],
   "source": [
    "# Download all raw data files\n",
    "!curl -O https://raw.githubusercontent.com/jqxcz/MATH3888/master/4932.protein.links.detailed.v11.5.txt\n",
    "!curl -O https://raw.githubusercontent.com/jqxcz/MATH3888/master/4932.protein.info.v11.5.txt\n",
    "!curl -O https://raw.githubusercontent.com/jqxcz/MATH3888/master/uniprot.tsv\n",
    "!curl -O https://raw.githubusercontent.com/jqxcz/MATH3888/master/essential.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing uniprot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table('uniprot.tsv',delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create columns that match for subcellular location\n",
    "\n",
    "We search the column `'Subcellular location [CC]'` for a substring match for each of the locations:\n",
    "\n",
    " - Mitochondrial protein\n",
    " - Cytoplasm protein\n",
    " - Membrane protein\n",
    " - Nucleus protein\n",
    " - Ribosomal protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = ['mitochon', 'cytoplasm', 'membrane', 'nucleus']\n",
    "\n",
    "for attr in attributes:\n",
    "    df[attr] = df['Subcellular location [CC]'].str.contains(attr, case=False)\n",
    "\n",
    "df['ribosomal'] = df['Protein names'].str.contains('ribosomal', case=False).fillna(False)\n",
    "df['ribosomal'] |= df['Protein names'].str.contains('ribosome', case=False).fillna(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the extra column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns='Subcellular location [CC]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a column to be the number of locations each protein is in.\n",
    " - Mitochondrial protein\n",
    " - Cytoplasm protein\n",
    " - Membrane protein\n",
    " - Nucleus protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['location_match'] = 0\n",
    "for attr in attributes:\n",
    "    df['location_match'] += df[attr]\n",
    "df['location_match'] = df['location_match'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataframe as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('uniprot_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in and doing initial processing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate common name for yeast proteins\n",
    "temp_df = pd.read_table(\"4932.protein.info.v11.5.txt\",delimiter='\\t')\n",
    "common_name = dict(temp_df.iloc[:,:2].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in preprocessed uniprot data\n",
    "uniprot_df = pd.read_csv('uniprot_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in essential node data\n",
    "essential_df = pd.read_csv('essential.csv', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Track studied proteins and their yeast name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's keep track of our studied proteins\n",
    "studied_proteins = {\n",
    "    'DIC1': '4932.YLR348C',\n",
    "    'RGT2': '4932.YDL138W',\n",
    "    'CBF5': '4932.YLR175W',\n",
    "    'EST2': '4932.YLR318W'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in dataframe for detailed protein links\n",
    "G0_dataframe = pd.read_table(\"./4932.protein.links.detailed.v11.5.txt\", delimiter=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reweight interactions. We:\n",
    "- Double the weight of experimental evidence\n",
    "- Halve the weight of textmining evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reweight edges of protein links\n",
    "keep_columns = [\n",
    "    'neighborhood', 'fusion', 'cooccurence', \n",
    "    'coexpression', 'experimental', 'database', \n",
    "    'textmining'\n",
    "]\n",
    "\n",
    "# Double weight of experimental evidence\n",
    "G0_dataframe['experimental'] *= 2\n",
    "\n",
    "# Half weight of textmining\n",
    "G0_dataframe['textmining'] //=2\n",
    "\n",
    "# Calculate combined score\n",
    "G0_dataframe.combined_score = G0_dataframe[keep_columns].max(axis=1)\n",
    "\n",
    "# Drop edges with combined score of zero\n",
    "G0_dataframe = G0_dataframe.drop(G0_dataframe[G0_dataframe.combined_score == 0].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all edges with combined score <= 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop edges with combined score of <= threshold score\n",
    "threshold_score = 600\n",
    "G0_dataframe = G0_dataframe.drop(\n",
    "    G0_dataframe[G0_dataframe.combined_score <= threshold_score].index\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check of data\n",
    "\n",
    "# G0_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create our networkx graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nx graph\n",
    "\n",
    "G0 = nx.from_pandas_edgelist(G0_dataframe, \n",
    "    source='protein1', \n",
    "    target='protein2', \n",
    "    edge_attr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we remove proteins that are not in cellular location of interest.\n",
    "\n",
    "We also remove ribosomal proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(irrelevant_proteins)=1334, len(ribosomal_proteins)=312\n",
      "len(G0.nodes)=5912\n",
      "len(G0.nodes)=4266\n"
     ]
    }
   ],
   "source": [
    "# Ignore certain nodes in the protein network, and also combine uniprot data\n",
    "\n",
    "# remove proteins that are not in a location that we are targeting\n",
    "# (we target membrane, mitochrondrial, cytoplasm and nucleus proteins)\n",
    "irrelevant_proteins = []\n",
    "\n",
    "# remove proteins that are ribosomal\n",
    "ribosomal_proteins = []\n",
    "\n",
    "row_index = {}\n",
    "\n",
    "for node in G0.nodes:\n",
    "    # Get row index of matching row for a protein\n",
    "    node_name = node.split('.')[1]\n",
    "    node_matches = uniprot_df['Gene Names'].str.contains(node_name).fillna(False)\n",
    "    node_uniprot = uniprot_df.index[node_matches][0]\n",
    "\n",
    "    if uniprot_df['location_match'][node_uniprot] == 0:\n",
    "        irrelevant_proteins.append(node)\n",
    "\n",
    "    elif uniprot_df['ribosomal'][node_uniprot] == True:\n",
    "        ribosomal_proteins.append(node)\n",
    "\n",
    "    else:\n",
    "        row_index[node] = node_uniprot\n",
    "\n",
    "print(f\"{len(irrelevant_proteins)=}, {len(ribosomal_proteins)=}\")\n",
    "print(f\"{len(G0.nodes)=}\")\n",
    "G0.remove_nodes_from(irrelevant_proteins)\n",
    "G0.remove_nodes_from(ribosomal_proteins)\n",
    "print(f\"{len(G0.nodes)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now remove essential proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(G0.nodes)=4266\n",
      "len(G0.nodes)=3271\n"
     ]
    }
   ],
   "source": [
    "# We remove essential proteins, but keep proteins we want to study\n",
    "essential_proteins_raw = essential_df[1].values\n",
    "essential_proteins = set(map(lambda x: '4932.' + x, essential_proteins_raw))\n",
    "essential_proteins -= set(studied_proteins.values())\n",
    "print(f\"{len(G0.nodes)=}\")\n",
    "G0.remove_nodes_from(list(essential_proteins))\n",
    "print(f\"{len(G0.nodes)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also remove proteins that are not in the largest connected component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(G0.nodes)=3172\n"
     ]
    }
   ],
   "source": [
    "# Let's only keep the largest connected component\n",
    "largest_component = max(list(nx.connected_components(G0)), key=len)\n",
    "G0 = G0.subgraph(largest_component)\n",
    "print(f\"{len(G0.nodes)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check - \n",
    "#   Check proteins we are interested in are still in the network\n",
    "\n",
    "for key, value in studied_proteins.items():\n",
    "    if value not in G0.nodes:\n",
    "        print(f\"ERROR: {key} was deleted from the network!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the location of each protein into the node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track location of nodes\n",
    "node_location = {}\n",
    "\n",
    "for node in G0.nodes:\n",
    "    if uniprot_df['mitochon'][row_index[node]] == 1:\n",
    "        node_location[node] = 'mitochondria'\n",
    "    elif uniprot_df['cytoplasm'][row_index[node]] == 1:\n",
    "        node_location[node] = 'cytoplasm'\n",
    "    elif uniprot_df['membrane'][row_index[node]] == 1:\n",
    "        node_location[node] = 'membrane'\n",
    "    elif uniprot_df['nucleus'][row_index[node]] == 1:\n",
    "        node_location[node] = 'nucleus'\n",
    "    else:\n",
    "        node_location[node] = 'other'\n",
    "\n",
    "nx.set_node_attributes(G0, values=node_location, name='location')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the direction of interaction to be those we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = {\n",
    "    'mitochondria': {'cytoplasm', 'mitochondria'},\n",
    "    'cytoplasm': {'nucleus', 'mitochondria', 'cytoplasm'},\n",
    "    'nucleus': {'nucleus'},\n",
    "    'membrane': {'mitochondria', 'membrane'}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct our directed graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "GX = nx.DiGraph()\n",
    "for edge in G0.edges:\n",
    "    if G0.nodes[edge[1]]['location'] in direction[G0.nodes[edge[0]]['location']]:\n",
    "        GX.add_edge(edge[0], edge[1])\n",
    "    if G0.nodes[edge[0]]['location'] in direction[G0.nodes[edge[1]]['location']]:\n",
    "        GX.add_edge(edge[1], edge[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our code for getting all paths between start and end.\n",
    "\n",
    "We allow for the abiity to set a mininum threshold for the least frequent protein amongst all paths returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(start, end, func, min_count=1, **kwargs):\n",
    "    paths = list(func(GX, \n",
    "            studied_proteins[start], \n",
    "            studied_proteins[end], **kwargs))\n",
    "    all_occurrences = collections.Counter(itertools.chain.from_iterable(paths))\n",
    "    return (p for p in paths if min(set(map(all_occurrences.get, p))) > min_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_paths(start, end, func, reverse=False, **kwargs):\n",
    "    out = lambda x: ', '.join(map(common_name.get, x))\n",
    "    if reverse:\n",
    "        out = lambda x: ', '.join(reversed(list(map(common_name.get, x))))\n",
    "    print('\\n'.join(sorted(map(out, get_paths(start, end, func, **kwargs)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIC1 to CBF5\n",
      "79515\n",
      "51174\n",
      "DIC1 to EST2\n",
      "34607\n",
      "10957\n",
      "RGT2 to CBF5\n",
      "142\n",
      "115\n",
      "RGT2 to EST2\n",
      "17\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "for start in ['DIC1', 'RGT2']:\n",
    "    for end in ['CBF5', 'EST2']:\n",
    "        try:\n",
    "            print(start, 'to', end)\n",
    "            num = sum(1 for _ in get_paths(start, end, nx.all_simple_paths, min_count=1, cutoff=5))\n",
    "            print(num)\n",
    "            print(sum(1 for _ in get_paths(start, end, nx.all_simple_paths, \n",
    "                min_count=math.sqrt(num)/2-2, cutoff=5)))\n",
    "        except Exception as e:\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
